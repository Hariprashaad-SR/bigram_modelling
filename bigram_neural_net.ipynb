{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **APPROACH - 2 (Neural networks)**"
      ],
      "metadata": {
        "id": "eExes20OAciK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0yvmQ-jpAXE0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "1rra8avVAbUg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = ['.'] + sorted(list(set(''.join(words))))"
      ],
      "metadata": {
        "id": "guG5nbusM19D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training set for the bigrams(x,y)\n",
        "\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words[:2]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs,chs[1:]):\n",
        "    bigram = (ch1,ch2)\n",
        "    print(f'bigram => {bigram}')\n",
        "    xs.append(lst.index(ch1))\n",
        "    ys.append(lst.index(ch2))\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "\n",
        "print(f'xs => {xs}\\nys => {ys}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2foX7XOnMznT",
        "outputId": "3219c4b2-f355-4080-a419-429a37bbdf39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram => ('.', 'e')\n",
            "bigram => ('e', 'm')\n",
            "bigram => ('m', 'm')\n",
            "bigram => ('m', 'a')\n",
            "bigram => ('a', '.')\n",
            "bigram => ('.', 'o')\n",
            "bigram => ('o', 'l')\n",
            "bigram => ('l', 'i')\n",
            "bigram => ('i', 'v')\n",
            "bigram => ('v', 'i')\n",
            "bigram => ('i', 'a')\n",
            "bigram => ('a', '.')\n",
            "xs => tensor([ 0,  5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1])\n",
            "ys => tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So from the above**\n",
        "\n",
        "* When we have 5, then the prob of 13 should be higher\n",
        "* When we have 13, then the prob of 1 should be higher\n",
        "* When we have 0, then the prob of 15 should be higher"
      ],
      "metadata": {
        "id": "BBGqTL4oNk2n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-sAq5R0zN1K1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Now to construct the neural net, we do not want to give the integer as the input, hence we use one hot encoding here**"
      ],
      "metadata": {
        "id": "x9biRf22OiEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "xenc = F.one_hot(xs, num_classes = 27)\n",
        "xenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC1i12k8OtNC",
        "outputId": "da738342-6878-4838-dec0-7fb032bb6a3c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc.shape # 12 words each with 27 len encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-6q08g8PA6e",
        "outputId": "418aacf9-225b-413c-900e-7e9f3e569ef6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(xenc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "LAG4LziePA8T",
        "outputId": "390dd431-9183-4ec7-a755-de5a33949f2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e487b058bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab7TUoljPQ4m",
        "outputId": "b1452d46-415e-4af6-aad1-264a817b7781"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = xenc.float()\n",
        "xenc.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6mlDf7WPyfp",
        "outputId": "91f908ac-07c4-4b29-9ba2-caf81ed81e3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **wx + b**"
      ],
      "metadata": {
        "id": "aoC0OSR0QBU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27,1)) # weights for each of the 27 inputs\n",
        "xenc @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAcnSSLLQFoo",
        "outputId": "ebb568e8-a91a-410f-8433-e9e17b6a44fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7922],\n",
              "        [ 1.1890],\n",
              "        [ 1.2677],\n",
              "        [ 1.2677],\n",
              "        [-1.6864],\n",
              "        [-1.7922],\n",
              "        [ 2.0186],\n",
              "        [-0.1836],\n",
              "        [ 0.1481],\n",
              "        [ 1.7487],\n",
              "        [ 0.1481],\n",
              "        [-1.6864]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(xenc @ W).shape      # 12 x 27   27 x 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgRybrfTQbRR",
        "outputId": "a32825e5-6061-4cba-fa5f-b68d86ab3a2b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27,27)) # weights for each of the 27 inputs\n",
        "xenc @ W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki5NCexNQrgl",
        "outputId": "a357ea47-beed-4e94-c221-d75c08c0c975"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4273,  0.7577, -0.4735,  2.5999, -0.9523,  0.3842,  0.4256, -0.7516,\n",
              "         -0.1715, -0.4516,  0.0844,  2.0764,  1.7518, -0.1073, -1.0219, -0.7053,\n",
              "          0.0298,  0.0879,  0.8379,  1.8357, -1.6790, -0.3657,  0.3723,  2.8612,\n",
              "          1.0607, -0.2642, -0.4960],\n",
              "        [ 1.9636,  1.0438,  0.6321, -0.1837, -1.0217,  0.8304, -1.2884, -0.0252,\n",
              "         -0.7813,  0.0837, -0.0693, -0.6243, -1.8408, -0.0358,  0.3546, -0.2751,\n",
              "          1.7959,  0.1627,  1.3015,  0.1567, -0.3212, -0.1348,  0.3621,  0.8569,\n",
              "          0.0631, -0.4971,  0.5086],\n",
              "        [ 0.7446, -0.5384, -0.2875, -1.0362, -1.4818,  0.6160, -0.5119,  0.2296,\n",
              "          1.6361, -0.3685,  0.0604, -1.4046, -0.3637, -0.8514,  0.0270,  0.6824,\n",
              "          0.2350,  0.4349,  0.8995,  0.0093,  1.9709,  0.0826, -1.4398,  1.6603,\n",
              "          0.4670,  0.6454,  1.0387],\n",
              "        [ 0.7446, -0.5384, -0.2875, -1.0362, -1.4818,  0.6160, -0.5119,  0.2296,\n",
              "          1.6361, -0.3685,  0.0604, -1.4046, -0.3637, -0.8514,  0.0270,  0.6824,\n",
              "          0.2350,  0.4349,  0.8995,  0.0093,  1.9709,  0.0826, -1.4398,  1.6603,\n",
              "          0.4670,  0.6454,  1.0387],\n",
              "        [ 0.3712, -2.6821, -0.1987,  1.5383, -2.0679,  0.4222, -1.3771,  1.5422,\n",
              "         -0.0294,  0.1573,  1.1064, -0.5316, -0.6652,  0.5738, -0.9531, -0.5826,\n",
              "         -1.0291, -1.1480,  1.6919, -0.2232, -0.2813, -0.8964,  0.5538,  0.5805,\n",
              "         -1.6450, -0.7404, -1.1342],\n",
              "        [ 0.4273,  0.7577, -0.4735,  2.5999, -0.9523,  0.3842,  0.4256, -0.7516,\n",
              "         -0.1715, -0.4516,  0.0844,  2.0764,  1.7518, -0.1073, -1.0219, -0.7053,\n",
              "          0.0298,  0.0879,  0.8379,  1.8357, -1.6790, -0.3657,  0.3723,  2.8612,\n",
              "          1.0607, -0.2642, -0.4960],\n",
              "        [ 1.2819,  0.8604, -0.9325, -1.5555, -0.0150,  0.5682,  1.4657, -2.0400,\n",
              "         -1.4171, -0.1550,  0.5365, -0.5605, -2.3672, -0.1702,  1.5749, -0.0315,\n",
              "         -1.0223, -1.3318, -0.7520, -0.2999, -0.1473,  0.9862, -0.3618,  1.2307,\n",
              "         -1.3131,  0.4470,  0.1007],\n",
              "        [-1.4161, -0.7438, -0.9536,  0.8573, -0.3611,  0.9344,  0.4632, -1.4898,\n",
              "          0.0596,  0.5672, -0.9237, -1.9874, -0.1306, -0.0817, -1.1626,  0.0974,\n",
              "         -1.2559,  1.8246,  1.1409, -0.9817,  0.1478, -0.8914,  0.1734, -0.4877,\n",
              "          0.6776,  1.9826, -0.6525],\n",
              "        [ 0.7884,  0.0445, -0.1118,  0.1642, -1.0773,  0.2533, -0.7638, -1.0823,\n",
              "          0.1323, -1.4493, -0.6192,  0.5536,  0.3946,  1.9563,  0.2605, -0.3430,\n",
              "          0.2817, -2.4508,  0.9992,  1.3004,  0.7108, -0.1522, -1.6539, -0.9137,\n",
              "         -2.0166, -0.7858, -0.3559],\n",
              "        [-0.6953,  2.4899, -1.1952, -0.9597,  0.2401, -0.7748,  1.6595, -0.7087,\n",
              "         -0.6088,  0.3430, -0.6552, -0.4116, -0.8740, -0.0795,  0.4742, -0.3900,\n",
              "          0.1622, -1.4240,  1.8338, -0.0673,  0.2414,  0.3346,  0.4784, -0.3700,\n",
              "         -0.6109,  0.6714, -0.1069],\n",
              "        [ 0.7884,  0.0445, -0.1118,  0.1642, -1.0773,  0.2533, -0.7638, -1.0823,\n",
              "          0.1323, -1.4493, -0.6192,  0.5536,  0.3946,  1.9563,  0.2605, -0.3430,\n",
              "          0.2817, -2.4508,  0.9992,  1.3004,  0.7108, -0.1522, -1.6539, -0.9137,\n",
              "         -2.0166, -0.7858, -0.3559],\n",
              "        [ 0.3712, -2.6821, -0.1987,  1.5383, -2.0679,  0.4222, -1.3771,  1.5422,\n",
              "         -0.0294,  0.1573,  1.1064, -0.5316, -0.6652,  0.5738, -0.9531, -0.5826,\n",
              "         -1.0291, -1.1480,  1.6919, -0.2232, -0.2813, -0.8964,  0.5538,  0.5805,\n",
              "         -1.6450, -0.7404, -1.1342]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(xenc @ W).shape      # 12 x 27   27 x 27"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abv-OBFKQxaL",
        "outputId": "67144b8b-df3c-4fb9-c2fc-982b6032be99"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Firing rate of the neuron\n",
        "(xenc @ W)[3, 13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbWjvYbAQ4y1",
        "outputId": "340d4130-47d8-4509-b46e-4ceaf8d78e61"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.8514)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will interpret the above xenc @ W as log counts and we need count from the above (exp()) => softmax( )"
      ],
      "metadata": {
        "id": "WgPRtpXLbp4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = (xenc @ W)\n",
        "logits = logits.exp()\n",
        "logits\n",
        "# everything became positive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSVYBYQfb4IB",
        "outputId": "e622400e-736e-44bb-a746-142ab30f6bf4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.5331,  2.1334,  0.6228, 13.4624,  0.3858,  1.4684,  1.5305,  0.4716,\n",
              "          0.8424,  0.6366,  1.0881,  7.9758,  5.7651,  0.8982,  0.3599,  0.4940,\n",
              "          1.0302,  1.0919,  2.3114,  6.2693,  0.1866,  0.6937,  1.4511, 17.4824,\n",
              "          2.8883,  0.7678,  0.6090],\n",
              "        [ 7.1249,  2.8400,  1.8816,  0.8321,  0.3600,  2.2943,  0.2757,  0.9751,\n",
              "          0.4578,  1.0873,  0.9330,  0.5357,  0.1587,  0.9648,  1.4256,  0.7595,\n",
              "          6.0251,  1.1767,  3.6748,  1.1696,  0.7253,  0.8739,  1.4364,  2.3559,\n",
              "          1.0651,  0.6083,  1.6629],\n",
              "        [ 2.1055,  0.5837,  0.7501,  0.3548,  0.2272,  1.8516,  0.5994,  1.2581,\n",
              "          5.1351,  0.6918,  1.0622,  0.2455,  0.6951,  0.4268,  1.0273,  1.9786,\n",
              "          1.2649,  1.5448,  2.4583,  1.0093,  7.1769,  1.0861,  0.2370,  5.2608,\n",
              "          1.5952,  1.9068,  2.8255],\n",
              "        [ 2.1055,  0.5837,  0.7501,  0.3548,  0.2272,  1.8516,  0.5994,  1.2581,\n",
              "          5.1351,  0.6918,  1.0622,  0.2455,  0.6951,  0.4268,  1.0273,  1.9786,\n",
              "          1.2649,  1.5448,  2.4583,  1.0093,  7.1769,  1.0861,  0.2370,  5.2608,\n",
              "          1.5952,  1.9068,  2.8255],\n",
              "        [ 1.4494,  0.0684,  0.8198,  4.6569,  0.1265,  1.5253,  0.2523,  4.6748,\n",
              "          0.9711,  1.1704,  3.0236,  0.5876,  0.5142,  1.7749,  0.3855,  0.5585,\n",
              "          0.3573,  0.3173,  5.4298,  0.8000,  0.7548,  0.4080,  1.7398,  1.7869,\n",
              "          0.1930,  0.4769,  0.3217],\n",
              "        [ 1.5331,  2.1334,  0.6228, 13.4624,  0.3858,  1.4684,  1.5305,  0.4716,\n",
              "          0.8424,  0.6366,  1.0881,  7.9758,  5.7651,  0.8982,  0.3599,  0.4940,\n",
              "          1.0302,  1.0919,  2.3114,  6.2693,  0.1866,  0.6937,  1.4511, 17.4824,\n",
              "          2.8883,  0.7678,  0.6090],\n",
              "        [ 3.6035,  2.3641,  0.3936,  0.2111,  0.9851,  1.7651,  4.3305,  0.1300,\n",
              "          0.2424,  0.8564,  1.7100,  0.5709,  0.0937,  0.8435,  4.8304,  0.9690,\n",
              "          0.3598,  0.2640,  0.4714,  0.7409,  0.8630,  2.6811,  0.6964,  3.4235,\n",
              "          0.2690,  1.5636,  1.1059],\n",
              "        [ 0.2427,  0.4753,  0.3854,  2.3568,  0.6969,  2.5456,  1.5891,  0.2254,\n",
              "          1.0614,  1.7633,  0.3970,  0.1371,  0.8776,  0.9215,  0.3127,  1.1023,\n",
              "          0.2848,  6.2004,  3.1296,  0.3747,  1.1593,  0.4101,  1.1893,  0.6140,\n",
              "          1.9691,  7.2616,  0.5208],\n",
              "        [ 2.1999,  1.0455,  0.8942,  1.1784,  0.3405,  1.2883,  0.4659,  0.3388,\n",
              "          1.1415,  0.2347,  0.5384,  1.7394,  1.4838,  7.0734,  1.2975,  0.7096,\n",
              "          1.3253,  0.0862,  2.7161,  3.6708,  2.0357,  0.8588,  0.1913,  0.4010,\n",
              "          0.1331,  0.4557,  0.7005],\n",
              "        [ 0.4989, 12.0601,  0.3027,  0.3830,  1.2713,  0.4608,  5.2566,  0.4923,\n",
              "          0.5440,  1.4092,  0.5193,  0.6626,  0.4173,  0.9236,  1.6068,  0.6770,\n",
              "          1.1761,  0.2407,  6.2574,  0.9349,  1.2730,  1.3973,  1.6134,  0.6907,\n",
              "          0.5429,  1.9570,  0.8986],\n",
              "        [ 2.1999,  1.0455,  0.8942,  1.1784,  0.3405,  1.2883,  0.4659,  0.3388,\n",
              "          1.1415,  0.2347,  0.5384,  1.7394,  1.4838,  7.0734,  1.2975,  0.7096,\n",
              "          1.3253,  0.0862,  2.7161,  3.6708,  2.0357,  0.8588,  0.1913,  0.4010,\n",
              "          0.1331,  0.4557,  0.7005],\n",
              "        [ 1.4494,  0.0684,  0.8198,  4.6569,  0.1265,  1.5253,  0.2523,  4.6748,\n",
              "          0.9711,  1.1704,  3.0236,  0.5876,  0.5142,  1.7749,  0.3855,  0.5585,\n",
              "          0.3573,  0.3173,  5.4298,  0.8000,  0.7548,  0.4080,  1.7398,  1.7869,\n",
              "          0.1930,  0.4769,  0.3217]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob = logits / logits.sum(dim = 1, keepdim = True)       #   Softmax function\n",
        "prob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CWVYZ4rcwCZ",
        "outputId": "05d15aa5-85a1-448b-9afa-9fa145e16966"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0206, 0.0287, 0.0084, 0.1808, 0.0052, 0.0197, 0.0206, 0.0063, 0.0113,\n",
              "         0.0086, 0.0146, 0.1071, 0.0774, 0.0121, 0.0048, 0.0066, 0.0138, 0.0147,\n",
              "         0.0310, 0.0842, 0.0025, 0.0093, 0.0195, 0.2348, 0.0388, 0.0103, 0.0082],\n",
              "        [0.1631, 0.0650, 0.0431, 0.0191, 0.0082, 0.0525, 0.0063, 0.0223, 0.0105,\n",
              "         0.0249, 0.0214, 0.0123, 0.0036, 0.0221, 0.0326, 0.0174, 0.1379, 0.0269,\n",
              "         0.0841, 0.0268, 0.0166, 0.0200, 0.0329, 0.0539, 0.0244, 0.0139, 0.0381],\n",
              "        [0.0464, 0.0129, 0.0165, 0.0078, 0.0050, 0.0408, 0.0132, 0.0277, 0.1132,\n",
              "         0.0153, 0.0234, 0.0054, 0.0153, 0.0094, 0.0226, 0.0436, 0.0279, 0.0341,\n",
              "         0.0542, 0.0223, 0.1582, 0.0239, 0.0052, 0.1160, 0.0352, 0.0420, 0.0623],\n",
              "        [0.0464, 0.0129, 0.0165, 0.0078, 0.0050, 0.0408, 0.0132, 0.0277, 0.1132,\n",
              "         0.0153, 0.0234, 0.0054, 0.0153, 0.0094, 0.0226, 0.0436, 0.0279, 0.0341,\n",
              "         0.0542, 0.0223, 0.1582, 0.0239, 0.0052, 0.1160, 0.0352, 0.0420, 0.0623],\n",
              "        [0.0412, 0.0019, 0.0233, 0.1325, 0.0036, 0.0434, 0.0072, 0.1330, 0.0276,\n",
              "         0.0333, 0.0860, 0.0167, 0.0146, 0.0505, 0.0110, 0.0159, 0.0102, 0.0090,\n",
              "         0.1545, 0.0228, 0.0215, 0.0116, 0.0495, 0.0508, 0.0055, 0.0136, 0.0092],\n",
              "        [0.0206, 0.0287, 0.0084, 0.1808, 0.0052, 0.0197, 0.0206, 0.0063, 0.0113,\n",
              "         0.0086, 0.0146, 0.1071, 0.0774, 0.0121, 0.0048, 0.0066, 0.0138, 0.0147,\n",
              "         0.0310, 0.0842, 0.0025, 0.0093, 0.0195, 0.2348, 0.0388, 0.0103, 0.0082],\n",
              "        [0.0992, 0.0651, 0.0108, 0.0058, 0.0271, 0.0486, 0.1192, 0.0036, 0.0067,\n",
              "         0.0236, 0.0471, 0.0157, 0.0026, 0.0232, 0.1329, 0.0267, 0.0099, 0.0073,\n",
              "         0.0130, 0.0204, 0.0237, 0.0738, 0.0192, 0.0942, 0.0074, 0.0430, 0.0304],\n",
              "        [0.0064, 0.0124, 0.0101, 0.0617, 0.0182, 0.0666, 0.0416, 0.0059, 0.0278,\n",
              "         0.0462, 0.0104, 0.0036, 0.0230, 0.0241, 0.0082, 0.0289, 0.0075, 0.1623,\n",
              "         0.0819, 0.0098, 0.0303, 0.0107, 0.0311, 0.0161, 0.0515, 0.1901, 0.0136],\n",
              "        [0.0637, 0.0303, 0.0259, 0.0341, 0.0099, 0.0373, 0.0135, 0.0098, 0.0330,\n",
              "         0.0068, 0.0156, 0.0504, 0.0430, 0.2048, 0.0376, 0.0205, 0.0384, 0.0025,\n",
              "         0.0786, 0.1063, 0.0589, 0.0249, 0.0055, 0.0116, 0.0039, 0.0132, 0.0203],\n",
              "        [0.0112, 0.2712, 0.0068, 0.0086, 0.0286, 0.0104, 0.1182, 0.0111, 0.0122,\n",
              "         0.0317, 0.0117, 0.0149, 0.0094, 0.0208, 0.0361, 0.0152, 0.0264, 0.0054,\n",
              "         0.1407, 0.0210, 0.0286, 0.0314, 0.0363, 0.0155, 0.0122, 0.0440, 0.0202],\n",
              "        [0.0637, 0.0303, 0.0259, 0.0341, 0.0099, 0.0373, 0.0135, 0.0098, 0.0330,\n",
              "         0.0068, 0.0156, 0.0504, 0.0430, 0.2048, 0.0376, 0.0205, 0.0384, 0.0025,\n",
              "         0.0786, 0.1063, 0.0589, 0.0249, 0.0055, 0.0116, 0.0039, 0.0132, 0.0203],\n",
              "        [0.0412, 0.0019, 0.0233, 0.1325, 0.0036, 0.0434, 0.0072, 0.1330, 0.0276,\n",
              "         0.0333, 0.0860, 0.0167, 0.0146, 0.0505, 0.0110, 0.0159, 0.0102, 0.0090,\n",
              "         0.1545, 0.0228, 0.0215, 0.0116, 0.0495, 0.0508, 0.0055, 0.0136, 0.0092]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwYJJmXcdLo0",
        "outputId": "e50040de-3ea8-4d0f-e7ed-f8cca409aef3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33SfiBDHdNMJ",
        "outputId": "73a2f42a-9be0-4241-bfa5-c7eee6417de1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Everything above is differentiable**"
      ],
      "metadata": {
        "id": "WntSFbQxdYTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SUMMARY ---------------------------------------------------"
      ],
      "metadata": {
        "id": "HrZxX6uxdcCV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **STEPS**\n",
        "*   Build the training set with xs and ys\n",
        "*   Initialize the weights for the neural net (no bias as it is a very simple network)\n",
        "*   One hot encode the input vectors\n",
        "*   then perform W * x\n",
        "*   Perform softmax and get the output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h4XYS5ebeZ4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'xs => {xs}\\nys => {ys}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHgJXfg5eBpq",
        "outputId": "62d789b0-810c-4e64-e20b-65c373369629"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xs => tensor([ 0,  5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1])\n",
            "ys => tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27,27), generator = g)"
      ],
      "metadata": {
        "id": "EN-EERGDd3DN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = F.one_hot(xs, num_classes = 27).float() # input to the network : one-hot encoding\n",
        "logits = (xenc @ W) # predict log counts\n",
        "logits = logits.exp() # counts\n",
        "prob = logits / logits.sum(dim = 1, keepdim = True) # softmax"
      ],
      "metadata": {
        "id": "elN9t6sBeGRP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W1m4fbpeGTi",
        "outputId": "3f6445c7-b8ef-4982-9ab9-384390db4cf4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlls = torch.zeros(5)\n",
        "\n",
        "for i in range(5):\n",
        "  # i-th bigram:\n",
        "  x = xs[i].item() # input character index\n",
        "  y = ys[i].item() # label character index\n",
        "  print('--------')\n",
        "  print(f'bigram example {i+1}: {lst[x]} -> {lst[y]}')\n",
        "  print('input to the neural net:', x)\n",
        "  print('output probabilities from the neural net:', prob[i])\n",
        "  print('label (actual next character):', y)\n",
        "  p = prob[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "  print('')\n",
        "\n",
        "print('=========')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRX0weIfdoR",
        "outputId": "69c9cf90-3111-4567-f710-46f955c7a151"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "bigram example 1: . -> e\n",
            "input to the neural net: 0\n",
            "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
            "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
            "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
            "label (actual next character): 5\n",
            "probability assigned by the net to the the correct character: 0.01228625513613224\n",
            "log likelihood: -4.399273872375488\n",
            "negative log likelihood: 4.399273872375488\n",
            "\n",
            "--------\n",
            "bigram example 2: e -> m\n",
            "input to the neural net: 5\n",
            "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
            "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
            "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the the correct character: 0.018050700426101685\n",
            "log likelihood: -4.014570713043213\n",
            "negative log likelihood: 4.014570713043213\n",
            "\n",
            "--------\n",
            "bigram example 3: m -> m\n",
            "input to the neural net: 13\n",
            "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the the correct character: 0.026691533625125885\n",
            "log likelihood: -3.623408794403076\n",
            "negative log likelihood: 3.623408794403076\n",
            "\n",
            "--------\n",
            "bigram example 4: m -> a\n",
            "input to the neural net: 13\n",
            "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label (actual next character): 1\n",
            "probability assigned by the net to the the correct character: 0.07367686182260513\n",
            "log likelihood: -2.6080665588378906\n",
            "negative log likelihood: 2.6080665588378906\n",
            "\n",
            "--------\n",
            "bigram example 5: a -> .\n",
            "input to the neural net: 1\n",
            "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
            "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
            "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
            "label (actual next character): 0\n",
            "probability assigned by the net to the the correct character: 0.014977526850998402\n",
            "log likelihood: -4.201204299926758\n",
            "negative log likelihood: 4.201204299926758\n",
            "\n",
            "=========\n",
            "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtXVhiw7gxSf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the above, we have a bad loss and one reason for it is the initial weights are completely random\n",
        "\n",
        "Now we will correct it by using a gradient descend setup"
      ],
      "metadata": {
        "id": "4uz6Jkd3hU6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly assigning the weights\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27,27), generator = g, requires_grad = True)"
      ],
      "metadata": {
        "id": "ciL3TSxfi-JQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "xenc = F.one_hot(xs, num_classes = 27).float() # input to the network : one-hot encoding\n",
        "logits = (xenc @ W) # predict log counts\n",
        "logits = logits.exp() # counts\n",
        "probs = logits / logits.sum(dim = 1, keepdim = True) # softmax"
      ],
      "metadata": {
        "id": "w9i77xWfhuuc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njAMb1b0jDxG",
        "outputId": "00c5fe65-0018-408c-969f-6df5da57bbb1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[torch.arange(12), ys]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-u4kMBPjb58",
        "outputId": "5bd860c5-dd91-4301-9a55-941358f50802"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150, 0.2378, 0.0526, 0.0135, 0.0367,\n",
              "        0.1056, 0.1059, 0.0150], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "loss = -probs[torch.arange(12), ys].log().mean()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WixeHuyjl0l",
        "outputId": "604f09cc-f40c-4d87-ce5d-7a92a6f78a92"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.2944, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "W.grad = None # zero gradient\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "Eu0DKmjxju04"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W.grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY_Z73h3j-sh",
        "outputId": "15a6b087-8363-4b32-984a-fb6b841c33e3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the weights\n",
        "W.data += -0.1 * W.grad"
      ],
      "metadata": {
        "id": "OMjpfgyTkMjv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJa1ydhdkIoc",
        "outputId": "07b3cfca-e422-4f34-c4c5-e9a091c49f42"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.29435658454895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Combining all**"
      ],
      "metadata": {
        "id": "JJ-vKsKaleWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs,ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs,chs[1:]):\n",
        "    xs.append(lst.index(ch1))\n",
        "    ys.append(lst.index(ch2))\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "\n",
        "print(f'number of samples : {num}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8TsHtFplhTL",
        "outputId": "03afaad3-e048-4e72-e3ff-0c01e4d6d80a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of samples : 228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27,27), generator = g, requires_grad = True)\n",
        "\n",
        "for epoch in range(101):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes = 27).float() # input to the network : one-hot encoding\n",
        "  logits = (xenc @ W) # predict log counts\n",
        "  logits = logits.exp() # counts\n",
        "  probs = logits / logits.sum(dim = 1, keepdim = True) # softmax\n",
        "\n",
        "  # loss\n",
        "  loss = -probs[torch.arange(num), ys].log().mean()\n",
        "  if epoch % 10 == 0:\n",
        "    print(f'Loss at epoch {epoch} : {loss.item() : .4f}')\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None  # zero gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update the weights\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jknqi8tMklTn",
        "outputId": "7c0b6469-6166-48d3-d2d2-483d15d09fed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch 0 :  3.7590\n",
            "Loss at epoch 10 :  2.6890\n",
            "Loss at epoch 20 :  2.5728\n",
            "Loss at epoch 30 :  2.5302\n",
            "Loss at epoch 40 :  2.5087\n",
            "Loss at epoch 50 :  2.4961\n",
            "Loss at epoch 60 :  2.4880\n",
            "Loss at epoch 70 :  2.4824\n",
            "Loss at epoch 80 :  2.4783\n",
            "Loss at epoch 90 :  2.4751\n",
            "Loss at epoch 100 :  2.4727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  out = []\n",
        "  ix = 0\n",
        "  while True:\n",
        "\n",
        "    # ----------\n",
        "    # BEFORE:\n",
        "    #p = P[ix]\n",
        "    # ----------\n",
        "    # NOW:\n",
        "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    # ----------\n",
        "\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(lst[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlNhwEcxof_g",
        "outputId": "c3c7620a-619d-49fd-f418-a4f3cb0c1e2b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junide.\n",
            "janasah.\n",
            "p.\n",
            "cfay.\n",
            "a.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The loss above is 2.47 is still the same as without a neural network for a bigram modelling**\n",
        "\n",
        "### But the neural net is more flexible and scalable compared to the manual work"
      ],
      "metadata": {
        "id": "Z9EpacuAme_n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwiirYWxmreC"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}
